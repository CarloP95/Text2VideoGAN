{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 2 Video \n",
    "\n",
    "In this notebook the necessary dependencies are imported to show the results of the Conditioning of MoCoGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies\n",
    "\n",
    "In the first cell the necessary models and states are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Constructor\n",
      "        -----------\n",
      "        The constructor of Generator_I takes 6 arguments, all optional.\n",
      "        \n",
      "        nc:         integer, default= 3\n",
      "            Num channels of the image to produce.\n",
      "        \n",
      "        ngf:        integer, default= 64\n",
      "            Parameter of the ConvTranspose2d Layers.\n",
      "        \n",
      "        nz:         integer, default= 60\n",
      "            Number of samples for the noise.\n",
      "            \n",
      "        ngpu:       integer, default= 1\n",
      "            Number of GPU on which the model will run.\n",
      "            \n",
      "        nClasses:   integer, default= 102\n",
      "            Number of classes on which the Embedding module will work.\n",
      "            \n",
      "        batch_size: integer, default = 16\n",
      "            Batch size for each argument that will be passed to the model.\n",
      "            \n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from mocogan.models import Generator_I, GRU\n",
    "\n",
    "print(Generator_I.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "Docstring has been added to the Generator_I class to make it possible to have more information about constructor and so on. \n",
    "\n",
    "As it is possible to see, all arguments are optional when working with the default parameters from the creators of MoCoGAN.\n",
    "\n",
    "Is enough to keep default parameters apart from `batch_size` and create the object Generator_I.\n",
    "\n",
    "Next cell will print the informations on how the model forwards information into layers.\n",
    "\n",
    "\n",
    "Be aware that the `forward` method of the Generator requires two arguments: `noise` and `label`.\n",
    "\n",
    "\n",
    "1. The first `Sequential` will put the labels passed as arguments (Action Classes) into an Embedding layer, giving as output $\\dfrac{nClasses}{16}$ features that will then pass through a Fully Connected Layer and then to a Rectified Linear Unit.\n",
    "\n",
    "2. Before passing input data into the second `Sequential` layer, input noise and output of the first pass will be concatenated. Then they are passed through a Fully Connected Layer that will output $ngf \\times 4$ features to the Transposed Convolution network.\n",
    "\n",
    "3. The last layer is equivalent to that of Vanilla MoCoGAN, it apply TransposedConvolution to get an image as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator_I(\n",
       "  (label_sequence): Sequential(\n",
       "    (0): Embedding(102, 6)\n",
       "    (1): Linear(in_features=6, out_features=60, bias=True)\n",
       "    (2): ReLU(inplace)\n",
       "  )\n",
       "  (combine_sequence): Sequential(\n",
       "    (0): Linear(in_features=272, out_features=256, bias=True)\n",
       "  )\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(60, 512, kernel_size=(6, 6), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_i = Generator_I(batch_size = 16)\n",
    "gen_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise\n",
    "\n",
    "In the following cells methods to create noise to provide as input to the network are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import skvideo.io\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "'''Define trained_path'''\n",
    "trained_path = !pwd\n",
    "trained_path = os.path.join(trained_path[0], \"mocogan\", \"trained_models\")\n",
    "\n",
    "'''Import variables from train.py'''\n",
    "img_size = 96\n",
    "nc = 3\n",
    "ndf = 64 # from dcgan\n",
    "ngf = 64\n",
    "d_E = 10\n",
    "hidden_size = 100 # guess\n",
    "d_C = 50\n",
    "d_M = d_E\n",
    "nz  = d_C + d_M\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "T = 16 # Hyperparameter for taking #Frames into discriminator.\n",
    "n_frames = 4 * 25 # 4 seconds of video\n",
    "\n",
    "batch_size = 16 #We only want one video to produce.\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "gru = GRU(d_E, hidden_size, gpu = cuda)\n",
    "\n",
    "# for input noises to generate fake video\n",
    "# note that noises are trimmed randomly from n_frames to T for efficiency\n",
    "def trim_noise(noise):\n",
    "    \n",
    "    start = np.random.randint(0, noise.size(1) - (T+1))\n",
    "    end = start + T\n",
    "    \n",
    "    return noise[:, start:end, :, :, :]\n",
    "\n",
    "\n",
    "def gen_z(n_frames, batch_size = batch_size):\n",
    "    \n",
    "    z_C = Variable(torch.randn(batch_size, d_C))\n",
    "    #  repeat z_C to (batch_size, n_frames, d_C)\n",
    "    z_C = z_C.unsqueeze(1).repeat(1, n_frames, 1)\n",
    "    eps = Variable(torch.randn(batch_size, d_E))\n",
    "    if cuda == True:\n",
    "        z_C, eps = z_C.cuda(), eps.cuda()\n",
    "\n",
    "    gru.initHidden(batch_size)\n",
    "    # notice that 1st dim of gru outputs is seq_len, 2nd is batch_size\n",
    "    z_M = gru(eps, n_frames).transpose(1, 0)\n",
    "    z = torch.cat((z_M, z_C), 2)  # z.size() => (batch_size, n_frames, nz)\n",
    "    \n",
    "    return z.view(batch_size, n_frames, nz, 1, 1)\n",
    "\n",
    "\n",
    "def save_video(fake_video, actionClass, baseDir):\n",
    "    outputdata = fake_video * 255\n",
    "    outputdata = outputdata.astype(np.uint8)\n",
    "    dir_path = os.path.join(baseDir, 'mocogan', 'generated_videos')\n",
    "    file_path = os.path.join(dir_path, f'{actionClass}.mp4')\n",
    "    skvideo.io.vwrite(file_path, outputdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  (gru): GRUCell(10, 100)\n",
       "  (drop): Dropout(p=0)\n",
       "  (linear): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load trained weights'''\n",
    "def load():\n",
    "    #dis_i.load_state_dict(torch.load(trained_path + '/Discriminator_I.model'))\n",
    "    #dis_v.load_state_dict(torch.load(trained_path + '/Discriminator_V.model'))\n",
    "    gen_i.load_state_dict(torch.load(trained_path + '/Generator_I_epoch-44.model'))\n",
    "    gru.load_state_dict(torch.load(trained_path + '/GRU_epoch-44.model'))\n",
    "    #optim_Di.load_state_dict(torch.load(trained_path + '/Discriminator_I.state'))\n",
    "    #optim_Dv.load_state_dict(torch.load(trained_path + '/Discriminator_V.state'))\n",
    "    #optim_Gi.load_state_dict(torch.load(trained_path + '/Generator_I.state'))\n",
    "    #optim_GRU.load_state_dict(torch.load(trained_path + '/GRU.state'))\n",
    "\n",
    "    \n",
    "load()\n",
    "'''Move models to GPU'''\n",
    "if cuda:\n",
    "    gen_i.cuda()\n",
    "    gru.cuda()\n",
    "    \n",
    "'''Change to evaluation mode'''\n",
    "gen_i.eval(); gru.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put input\n",
    "\n",
    "In the following cell you can decide to put a number between 1 and 101 or to put a class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a number between 1 and 101 or just put a name.\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    actionClass = int(input(\"Choose a number between 1 and 101 or just put a name.\\n\"))\n",
    "    assert actionClass > 0 and actionClass < 102\n",
    "    \n",
    "except ValueError as _:\n",
    "    #Check if actionClass is contained into the dictionary\n",
    "    pass\n",
    "\n",
    "except AssertionError as _:\n",
    "    print(\"Please put a Number between 1 and 101.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "fakeLabels = torch.randint(0, 101, tuple([batch_size]), dtype=torch.long)\n",
    "fakeLabels[0] = actionClass\n",
    "\n",
    "label = fakeLabels#torch.tensor([actionClass], dtype=torch.long)\n",
    "\n",
    "if cuda:\n",
    "    label = label.cuda()\n",
    "\n",
    "batch_size = 16\n",
    "    \n",
    "Z = gen_z(n_frames, batch_size)  # Z.size() => (batch_size, n_frames, nz, 1, 1)\n",
    "# trim => (batch_size, T, nz, 1, 1)\n",
    "Z = trim_noise(Z)\n",
    "# generate videos\n",
    "Z = Z.contiguous().view(batch_size*T, nz, 1, 1)\n",
    "\n",
    "fake_videos = gen_i(Z, label)\n",
    "print(fake_videos.shape)\n",
    "fake_videos = fake_videos.view(batch_size, T, nc, img_size, img_size)\n",
    "# transpose => (batch_size, nc, T, img_size, img_size)\n",
    "fake_videos = fake_videos.transpose(2, 1)\n",
    "# img sampling\n",
    "fake_img = fake_videos[:, :, np.random.randint(0, T), :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDir = !pwd\n",
    "currentDir = currentDir[0]\n",
    "\n",
    "for idx, video in enumerate(fake_videos):\n",
    "    save_video(video.data.cpu().numpy().transpose(1, 2, 3, 0), f\"{actionClass}-{idx}\", currentDir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
