{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of MoCoGAN\n",
    "---\n",
    "In this Notebook is covered the topic of training the MoCoGAN and the fine tuning of this network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining global variables and imports\n",
    "---\n",
    "In the below cell all of the imports that are needed to the training and some global variables like `device` to use the GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import skvideo.io\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim, cuda as cu, device, manual_seed\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Lambda, Compose\n",
    "\n",
    "## Try to avoid problems with dataloader.\n",
    "\"\"\"\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "try:\n",
    "    mp.set_start_method('forkserver')\n",
    "    #mp.set_start_method('spawn')\n",
    "except RuntimeError as errs:\n",
    "    print(errs)\n",
    "\"\"\"\n",
    "##########################################\n",
    "\n",
    "'''Import variables from train.py'''\n",
    "img_size = 96\n",
    "nc = 3\n",
    "ndf = 64 # from dcgan\n",
    "ngf = 64\n",
    "d_E = 10\n",
    "hidden_size = 100 # guess\n",
    "d_C = 50\n",
    "d_M = d_E\n",
    "nz  = d_C + d_M\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "T = 16 # Hyperparameter for taking #Frames into discriminator.\n",
    "\n",
    "ngpu       = 1\n",
    "batch_size = 16\n",
    "n_iter     = 120000\n",
    "pre_train  = False\n",
    "\n",
    "## Addition for training on UCF-101\n",
    "n_epochs_saveV      = 1\n",
    "n_epochs_display    = 1\n",
    "n_epochs_check      = 1\n",
    "max_frame           = 25\n",
    "cuda                = True #For compatibility with old version of MocoGan\n",
    "#### End of additions\n",
    "\n",
    "\n",
    "seed = 0\n",
    "manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = device(\"cuda\" if cu.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of Models\n",
    "From the `models` module, let's import all of the models and let's load the previous state for fine tuning.\n",
    "\n",
    "Then models are moved into the device chosen in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./mocogan/\")\n",
    "from models import Discriminator_I, Discriminator_V, Generator_I ,GRU, UCF_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./mocogan/models.py:211: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(params)\n",
      "./mocogan/models.py:221: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(params, 0)\n"
     ]
    }
   ],
   "source": [
    "'''Create the objects for Discriminator_(I|V), GRU and Generator_I'''\n",
    "gen_i = Generator_I(nc, ngf, nz, ngpu = 1, batch_size= batch_size)\n",
    "gru = GRU(d_E, hidden_size, gpu = cu.is_available())\n",
    "dis_i = Discriminator_I(nc, ndf, ngpu = 1)\n",
    "dis_v = Discriminator_V(nc, ndf, T = T, ngpu = 1)\n",
    "gru.initWeight()\n",
    "\n",
    "'''Move objects into the device chosen'''\n",
    "''' adjust to cuda '''\n",
    "if cuda == True:\n",
    "    dis_i.cuda()\n",
    "    dis_v.cuda()\n",
    "    gen_i.cuda()\n",
    "    gru.cuda()\n",
    "    criterion.cuda()\n",
    "\n",
    "'''Optimizer Settings and Optimizer'''\n",
    "lr = 0.0002\n",
    "betas=(0.5)\n",
    "optim_Di  = optim.Adam(dis_i.parameters(), lr=lr, betas=betas)\n",
    "optim_Dv  = optim.Adam(dis_v.parameters(), lr=lr, betas=betas)\n",
    "optim_Gi  = optim.Adam(gen_i.parameters(), lr=lr, betas=betas)\n",
    "optim_GRU = optim.Adam(gru.parameters(),   lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "---\n",
    "In the cells below, the dataloader will be defined and also all of the transformation that will be applied to the videos before taking them into a batch.\n",
    "To apply transformation to videos, since torchvision does not have a set of methods for this task, a repository called `torch_videovision` has been forked and modified to support transformation on numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlo/anaconda3/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "from torch_videovision.videotransforms.video_transforms import ColorJitter, TemporalCrop    \n",
    "from torch_videovision.videotransforms.volume_transforms import ToTensor, ClipToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = !pwd\n",
    "current_path = str(current_path[0])\n",
    "resized_path = os.path.join(current_path, \"mocogan\", 'resized_data')\n",
    "files = glob(resized_path+'/*/*')\n",
    "\n",
    "'''\n",
    "transformation = Compose([Lambda(lambda video: video.transpose(3, 0, 1, 2)/255.0),\n",
    "                            Lambda(lambda video: video[ : , : max_frame, :, : ]),\n",
    "                            Lambda(lambda video: torch.FloatTensor(video))])\n",
    "'''\n",
    "\n",
    "transformation = Compose([ ClipToTensor(numpy = True),\n",
    "                            TemporalCrop(max_frame),\n",
    "                            ToTensor()])\n",
    "\n",
    "\n",
    "dataset = UCF_101(resized_path, supportedExtensions= [\"mp4\"], transform= transformation)\n",
    "dataloader = DataLoader(dataset, batch_size= batch_size, num_workers= 8, shuffle= True, pin_memory= True, drop_last= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "---\n",
    "\n",
    "In the cells below, first some methods to generate the Noise needed for the GAN is defined, then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Utilities'''\n",
    "def trim(video):\n",
    "    start = np.random.randint(0, video.shape[1] - (T+1))\n",
    "    end = start + T\n",
    "    return video[:, start:end, :, :]\n",
    "\n",
    "# for input noises to generate fake video\n",
    "# note that noises are trimmed randomly from n_frames to T for efficiency\n",
    "def trim_noise(noise):\n",
    "    #print(\"-----TRIMMING NOISE-----\")\n",
    "    #print(f\"Noise Size: {noise.size()}\")\n",
    "    start = np.random.randint(0, noise.size(1) - (T+1))\n",
    "    end = start + T\n",
    "    #print(\"-----END OF TRIMMING NOISE-----\")\n",
    "    return noise[:, start:end, :, :, :]\n",
    "\n",
    "\n",
    "''' calc grad of models '''\n",
    "\n",
    "def bp_i(inputs, y, retain=False):\n",
    "    label = (torch.FloatTensor()).cuda()\n",
    "    label.resize_(inputs.size(0)).fill_(y)\n",
    "    labelv = Variable(label)\n",
    "    outputs = dis_i(inputs)\n",
    "    err = criterion(outputs, labelv)\n",
    "    err.backward(retain_graph=retain)\n",
    "    toReturnErr = err.data[0] if err.size() == torch.Tensor().size() else err.item()\n",
    "    return toReturnErr, outputs.data.mean()\n",
    "\n",
    "def bp_v(inputs, labels, y, retain=False):\n",
    "    #print(\"----BackPropagate_V-----\")\n",
    "    #print(inputs.size())\n",
    "    label = (torch.FloatTensor()).cuda()\n",
    "    try:\n",
    "        label.resize_(inputs.size(0)).fill_(y)\n",
    "\n",
    "    except RuntimeError as _:\n",
    "        # Dimension of y does not allow to use fill_\n",
    "        assert(inputs.size(0) == y.size(0))\n",
    "        label = (torch.FloatTensor(y)).cuda()\n",
    "\n",
    "    labelv = Variable(label)\n",
    "    outputs = dis_v(inputs, labels)\n",
    "    err = criterion(outputs, labelv)\n",
    "    err.backward(retain_graph=retain)\n",
    "    toReturnErr = err.data[0] if err.size() == torch.Tensor().size() else err.item()\n",
    "    #print(\"----End of BackPropagate_V-----\")\n",
    "    return toReturnErr, outputs.data.mean()\n",
    "\n",
    "\n",
    "''' gen input noise for fake video '''\n",
    "\n",
    "def gen_z(n_frames, batch_size = batch_size):\n",
    "    #print(\"----Generating Z-----\")\n",
    "    #print(f\"N_FRAMES: {n_frames}\")\n",
    "    #print(f\"BATCH_SIZE: {batch_size}\")\n",
    "    #print(f\"D_C: {d_C}\")\n",
    "    #print(f\"D_E: {d_E}\")\n",
    "    #print(f\"nz: {nz}\")\n",
    "    z_C = Variable(torch.randn(batch_size, d_C))\n",
    "    #  repeat z_C to (batch_size, n_frames, d_C)\n",
    "    z_C = z_C.unsqueeze(1).repeat(1, n_frames, 1)\n",
    "    eps = Variable(torch.randn(batch_size, d_E))\n",
    "    if cuda == True:\n",
    "        z_C, eps = z_C.cuda(), eps.cuda()\n",
    "\n",
    "    gru.initHidden(batch_size)\n",
    "    # notice that 1st dim of gru outputs is seq_len, 2nd is batch_size\n",
    "    z_M = gru(eps, n_frames).transpose(1, 0)\n",
    "    z = torch.cat((z_M, z_C), 2)  # z.size() => (batch_size, n_frames, nz)\n",
    "    #print(\"----End Generating Z-----\")\n",
    "    return z.view(batch_size, n_frames, nz, 1, 1)\n",
    "\n",
    "''' prepare for train '''\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    d = math.floor(s / ((60**2)*24))\n",
    "    h = math.floor(s / (60**2)) - d*24\n",
    "    m = math.floor(s / 60) - h*60 - d*24*60\n",
    "    s = s - m*60 - h*(60**2) - d*24*(60**2)\n",
    "    return '%dd %dh %dm %ds' % (d, h, m, s)\n",
    "\n",
    "trained_path = os.path.join(current_path, \"mocogan\", 'trained_models')\n",
    "def checkpoint(model, optimizer, epoch):\n",
    "    filename = os.path.join(trained_path, '%s_epoch-%d' % (model.__class__.__name__, epoch))\n",
    "    torch.save(model.state_dict(), filename + '.model')\n",
    "    torch.save(optimizer.state_dict(), filename + '.state')\n",
    "\n",
    "def save_video(fake_video, epoch):\n",
    "    outputdata = fake_video * 255\n",
    "    outputdata = outputdata.astype(np.uint8)\n",
    "    dir_path = os.path.join(current_path, 'mocogan', 'generated_videos')\n",
    "    file_path = os.path.join(dir_path, 'fakeVideo_epoch-%d.mp4' % (epoch))\n",
    "    skvideo.io.vwrite(file_path, outputdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' train models '''\n",
    "def train():\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Starting training: CUDA is { 'On' if cuda == True else 'Off'}\")\n",
    "\n",
    "    for epoch in range(1, n_iter+1):\n",
    "        ''' prepare real images '''\n",
    "        # real_videos.size() => (batch_size, nc, T, img_size, img_size)\n",
    "\n",
    "        # Get data iterator\n",
    "        fake_label = 0\n",
    "        true_label = [val for val in range(1, 102)]\n",
    "        data_iter = iter(dataloader) #Iterator\n",
    "        data_len = len(dataloader) #Num Batches\n",
    "        data_i = 0\n",
    "\n",
    "        while data_i < data_len:\n",
    "\n",
    "            try:\n",
    "          \n",
    "                (real_videos, labels) = next(data_iter)\n",
    "                  \n",
    "                print(f\"\\r--------Batch {data_i}/{data_len}---------\", end = \"\")\n",
    "                #for (key, val) in dataset.class_to_idx.items():\n",
    "                #    if ( val in labels.tolist() ):\n",
    "                #        #print(key)\n",
    "                #        pass\n",
    "\n",
    "                if cuda == True:\n",
    "                    real_videos = real_videos.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                real_videos = Variable(real_videos)\n",
    "                real_img = real_videos[:, :, np.random.randint(0, T), :, :]\n",
    "\n",
    "                ''' prepare fake images '''\n",
    "                # note that n_frames is sampled from video length distribution\n",
    "                if (len(dataset.videoLengths) > 0):\n",
    "                    randomVideo = list(dataset.videoLengths)[np.random.randint(0, len(dataset.videoLengths))]\n",
    "                    n_frames = dataset.videoLengths[randomVideo]\n",
    "          \n",
    "                else: # Use this for first iterations, when dataset.videoLengths is not yet updated.\n",
    "                    n_frames = T + 2 + np.random.randint(0, real_videos.size()[2])\n",
    "          \n",
    "                Z = gen_z(n_frames, batch_size)  # Z.size() => (batch_size, n_frames, nz, 1, 1)\n",
    "                # trim => (batch_size, T, nz, 1, 1)\n",
    "                Z = trim_noise(Z)\n",
    "                # generate videos\n",
    "                Z = Z.contiguous().view(batch_size*T, nz, 1, 1)\n",
    "                \n",
    "                fake_videos = gen_i(Z, labels)\n",
    "                fake_videos = fake_videos.view(batch_size, T, nc, img_size, img_size)\n",
    "                # transpose => (batch_size, nc, T, img_size, img_size)\n",
    "                fake_videos = fake_videos.transpose(2, 1)\n",
    "                # img sampling\n",
    "                fake_img = fake_videos[:, :, np.random.randint(0, T), :, :]\n",
    "\n",
    "                ''' train discriminators '''\n",
    "                # video\n",
    "                dis_v.zero_grad()\n",
    "                randomStartFrameIdx = np.random.randint(0, real_videos.size()[2] - T - 1)\n",
    "                #print(\"-----INFOS-----\")\n",
    "                #print(f\"RandomStartFrame:{randomStartFrameIdx}\")\n",
    "                #print(f\"Video Size:{real_videos.size()}\")\n",
    "                #print(\"-----END OF INFOS-----\")\n",
    "                croppedRealVideos = real_videos[:,:,randomStartFrameIdx: randomStartFrameIdx + T, :, :]\n",
    "                err_Dv_real, Dv_real_mean = bp_v(croppedRealVideos, labels, 0.9)\n",
    "                #err_Dv_real, Dv_real_mean = bp_v(croppedRealVideos, labels.type(torch.FloatTensor) / len(dictClassesIdx))\n",
    "                err_Dv_fake, Dv_fake_mean = bp_v(fake_videos.detach(), labels, fake_label)\n",
    "                err_Dv = err_Dv_real + err_Dv_fake\n",
    "                optim_Dv.step()\n",
    "                # image\n",
    "                dis_i.zero_grad()\n",
    "                err_Di_real, Di_real_mean = bp_i(real_img, 0.9)\n",
    "                err_Di_fake, Di_fake_mean = bp_i(fake_img.detach(), fake_label)\n",
    "                err_Di = err_Di_real + err_Di_fake\n",
    "                optim_Di.step()\n",
    "\n",
    "\n",
    "                ''' train generators '''\n",
    "                gen_i.zero_grad()\n",
    "                gru.zero_grad()\n",
    "                # video. notice retain=True for back prop twice\n",
    "                err_Gv, _ = bp_v(fake_videos, labels, 0.9, retain=True)\n",
    "                # images\n",
    "                err_Gi, _ = bp_i(fake_img, 0.9)\n",
    "                optim_Gi.step()\n",
    "                optim_GRU.step()\n",
    "\n",
    "                '''Increment index for Batch'''\n",
    "                data_i = data_i + 1\n",
    "          \n",
    "                '''Cool down the Hardware'''\n",
    "                time.sleep(1)\n",
    "          \n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                save_video(fake_videos[0].data.cpu().numpy().transpose(1, 2, 3, 0), epoch)\n",
    "                checkpoint(dis_i, optim_Di, epoch)\n",
    "                checkpoint(dis_v, optim_Dv, epoch)\n",
    "                checkpoint(gen_i, optim_Gi, epoch)\n",
    "                checkpoint(gru,   optim_GRU, epoch)\n",
    "\n",
    "        if epoch % n_epochs_display == 0:\n",
    "            print('[%d/%d] (%s) Loss_Di: %.4f Loss_Dv: %.4f Loss_Gi: %.4f Loss_Gv: %.4f Di_real_mean %.4f Di_fake_mean %.4f Dv_real_mean %.4f Dv_fake_mean %.4f'\n",
    "                  % (epoch, n_iter, timeSince(start_time), err_Di, err_Dv, err_Gi, err_Gv, Di_real_mean, Di_fake_mean, Dv_real_mean, Dv_fake_mean))\n",
    "\n",
    "        if epoch % n_epochs_saveV == 0:\n",
    "            save_video(fake_videos[0].data.cpu().numpy().transpose(1, 2, 3, 0), epoch)\n",
    "\n",
    "        if epoch % n_epochs_check == 0:\n",
    "            checkpoint(dis_i, optim_Di, epoch)\n",
    "            checkpoint(dis_v, optim_Dv, epoch)\n",
    "            checkpoint(gen_i, optim_Gi, epoch)\n",
    "            checkpoint(gru,   optim_GRU, epoch)\n",
    "          \n",
    "        '''Cool down the Hardware'''\n",
    "        time.sleep(10)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Previous State\n",
    "---\n",
    "If wanted, the following cell can be used to load the previous state of a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' use pre-trained models '''\n",
    "\n",
    "def load():\n",
    "    dis_i.load_state_dict(torch.load(trained_path + '/Discriminator_I.model'))\n",
    "    dis_v.load_state_dict(torch.load(trained_path + '/Discriminator_V.model'))\n",
    "    gen_i.load_state_dict(torch.load(trained_path + '/Generator_I.model'))\n",
    "    gru.load_state_dict(torch.load(trained_path + '/GRU.model'))\n",
    "    optim_Di.load_state_dict(torch.load(trained_path + '/Discriminator_I.state'))\n",
    "    optim_Dv.load_state_dict(torch.load(trained_path + '/Discriminator_V.state'))\n",
    "    optim_Gi.load_state_dict(torch.load(trained_path + '/Generator_I.state'))\n",
    "    optim_GRU.load_state_dict(torch.load(trained_path + '/GRU.state'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, start training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training: CUDA is On\n",
      "--------Batch 831/832---------[1/120000] (0d 0h 24m 32s) Loss_Di: 0.5732 Loss_Dv: 0.6753 Loss_Gi: 3.7930 Loss_Gv: 3.4407 Di_real_mean 0.7657 Di_fake_mean 0.1124 Dv_real_mean 0.9776 Dv_fake_mean 0.1700\n",
      "--------Batch 831/832---------[2/120000] (0d 0h 49m 16s) Loss_Di: 0.6845 Loss_Dv: 0.6531 Loss_Gi: 4.6168 Loss_Gv: 2.4914 Di_real_mean 0.8604 Di_fake_mean 0.2273 Dv_real_mean 0.6257 Dv_fake_mean 0.0876\n",
      "--------Batch 831/832---------[3/120000] (0d 1h 14m 0s) Loss_Di: 0.4681 Loss_Dv: 0.4443 Loss_Gi: 3.1862 Loss_Gv: 3.3579 Di_real_mean 0.7976 Di_fake_mean 0.0775 Dv_real_mean 0.9657 Dv_fake_mean 0.0512\n",
      "--------Batch 831/832---------[4/120000] (0d 1h 38m 45s) Loss_Di: 1.1685 Loss_Dv: 0.5864 Loss_Gi: 7.3082 Loss_Gv: 3.6898 Di_real_mean 0.3689 Di_fake_mean 0.0001 Dv_real_mean 0.8403 Dv_fake_mean 0.1711\n",
      "--------Batch 831/832---------[5/120000] (0d 2h 3m 31s) Loss_Di: 0.4278 Loss_Dv: 0.6922 Loss_Gi: 5.5002 Loss_Gv: 1.6240 Di_real_mean 0.9366 Di_fake_mean 0.0256 Dv_real_mean 0.6356 Dv_fake_mean 0.0654\n",
      "--------Batch 831/832---------[6/120000] (0d 2h 28m 16s) Loss_Di: 0.4675 Loss_Dv: 0.3763 Loss_Gi: 3.7365 Loss_Gv: 5.7515 Di_real_mean 0.9134 Di_fake_mean 0.0649 Dv_real_mean 0.8677 Dv_fake_mean 0.0080\n",
      "--------Batch 831/832---------[7/120000] (0d 2h 53m 10s) Loss_Di: 0.5225 Loss_Dv: 0.4843 Loss_Gi: 3.2470 Loss_Gv: 4.0231 Di_real_mean 0.7010 Di_fake_mean 0.0345 Dv_real_mean 0.8855 Dv_fake_mean 0.1087\n",
      "--------Batch 831/832---------[8/120000] (0d 3h 18m 6s) Loss_Di: 0.6320 Loss_Dv: 0.9628 Loss_Gi: 7.8026 Loss_Gv: 5.0249 Di_real_mean 0.6105 Di_fake_mean 0.0005 Dv_real_mean 0.9783 Dv_fake_mean 0.3632\n",
      "--------Batch 831/832---------[9/120000] (0d 3h 42m 51s) Loss_Di: 0.5208 Loss_Dv: 0.4493 Loss_Gi: 4.4535 Loss_Gv: 3.3364 Di_real_mean 0.9693 Di_fake_mean 0.0868 Dv_real_mean 0.7985 Dv_fake_mean 0.0402\n",
      "--------Batch 831/832---------[10/120000] (0d 4h 7m 36s) Loss_Di: 0.4056 Loss_Dv: 0.3954 Loss_Gi: 4.4188 Loss_Gv: 4.7771 Di_real_mean 0.9326 Di_fake_mean 0.0234 Dv_real_mean 0.9192 Dv_fake_mean 0.0144\n",
      "--------Batch 831/832---------[11/120000] (0d 4h 32m 21s) Loss_Di: 0.4930 Loss_Dv: 0.7424 Loss_Gi: 2.9114 Loss_Gv: 2.6940 Di_real_mean 0.7632 Di_fake_mean 0.0652 Dv_real_mean 0.9837 Dv_fake_mean 0.2151\n",
      "--------Batch 831/832---------[12/120000] (0d 4h 57m 7s) Loss_Di: 0.4310 Loss_Dv: 0.7391 Loss_Gi: 4.5138 Loss_Gv: 5.4768 Di_real_mean 0.9295 Di_fake_mean 0.0432 Dv_real_mean 0.9665 Dv_fake_mean 0.2594\n",
      "--------Batch 831/832---------[13/120000] (0d 5h 21m 51s) Loss_Di: 0.9818 Loss_Dv: 0.6419 Loss_Gi: 7.1599 Loss_Gv: 4.1950 Di_real_mean 0.9888 Di_fake_mean 0.3486 Dv_real_mean 0.9769 Dv_fake_mean 0.1594\n",
      "--------Batch 831/832---------[14/120000] (0d 5h 46m 36s) Loss_Di: 0.6943 Loss_Dv: 0.4880 Loss_Gi: 5.4020 Loss_Gv: 4.4341 Di_real_mean 0.8836 Di_fake_mean 0.2855 Dv_real_mean 0.9442 Dv_fake_mean 0.0757\n",
      "--------Batch 831/832---------[15/120000] (0d 6h 11m 20s) Loss_Di: 0.3932 Loss_Dv: 0.5700 Loss_Gi: 6.3215 Loss_Gv: 4.4542 Di_real_mean 0.9531 Di_fake_mean 0.0075 Dv_real_mean 0.9044 Dv_fake_mean 0.1720\n",
      "--------Batch 831/832---------[16/120000] (0d 6h 36m 5s) Loss_Di: 0.3993 Loss_Dv: 0.5099 Loss_Gi: 4.9223 Loss_Gv: 3.5147 Di_real_mean 0.8240 Di_fake_mean 0.0104 Dv_real_mean 0.7347 Dv_fake_mean 0.0243\n",
      "--------Batch 831/832---------[17/120000] (0d 7h 0m 48s) Loss_Di: 0.4430 Loss_Dv: 0.6542 Loss_Gi: 4.2069 Loss_Gv: 2.3914 Di_real_mean 0.9493 Di_fake_mean 0.0729 Dv_real_mean 0.7192 Dv_fake_mean 0.0661\n",
      "--------Batch 831/832---------[18/120000] (0d 7h 25m 32s) Loss_Di: 0.3919 Loss_Dv: 0.4856 Loss_Gi: 4.2418 Loss_Gv: 2.6935 Di_real_mean 0.8196 Di_fake_mean 0.0077 Dv_real_mean 0.9023 Dv_fake_mean 0.0839\n",
      "--------Batch 831/832---------[19/120000] (0d 7h 50m 15s) Loss_Di: 0.4679 Loss_Dv: 0.5469 Loss_Gi: 4.2924 Loss_Gv: 2.3711 Di_real_mean 0.7305 Di_fake_mean 0.0027 Dv_real_mean 0.6841 Dv_fake_mean 0.0145\n",
      "--------Batch 831/832---------[20/120000] (0d 8h 14m 57s) Loss_Di: 0.4172 Loss_Dv: 0.4615 Loss_Gi: 3.8962 Loss_Gv: 3.3961 Di_real_mean 0.7978 Di_fake_mean 0.0099 Dv_real_mean 0.7562 Dv_fake_mean 0.0064\n",
      "--------Batch 831/832---------[21/120000] (0d 8h 39m 40s) Loss_Di: 0.4593 Loss_Dv: 0.4898 Loss_Gi: 5.1728 Loss_Gv: 3.5692 Di_real_mean 0.9751 Di_fake_mean 0.0345 Dv_real_mean 0.8174 Dv_fake_mean 0.0169\n",
      "--------Batch 831/832---------[22/120000] (0d 9h 4m 21s) Loss_Di: 0.3861 Loss_Dv: 0.4669 Loss_Gi: 4.3366 Loss_Gv: 4.5374 Di_real_mean 0.8183 Di_fake_mean 0.0094 Dv_real_mean 0.9757 Dv_fake_mean 0.0581\n",
      "--------Batch 831/832---------[23/120000] (0d 9h 29m 4s) Loss_Di: 0.3504 Loss_Dv: 0.5847 Loss_Gi: 5.9336 Loss_Gv: 3.6173 Di_real_mean 0.9194 Di_fake_mean 0.0033 Dv_real_mean 0.9776 Dv_fake_mean 0.0965\n",
      "--------Batch 831/832---------[24/120000] (0d 9h 53m 46s) Loss_Di: 0.4237 Loss_Dv: 0.3723 Loss_Gi: 4.4169 Loss_Gv: 4.6504 Di_real_mean 0.7673 Di_fake_mean 0.0065 Dv_real_mean 0.9145 Dv_fake_mean 0.0197\n",
      "--------Batch 831/832---------[25/120000] (0d 10h 18m 28s) Loss_Di: 0.6041 Loss_Dv: 0.3772 Loss_Gi: 2.0742 Loss_Gv: 6.8250 Di_real_mean 0.6444 Di_fake_mean 0.0122 Dv_real_mean 0.8931 Dv_fake_mean 0.0020\n",
      "--------Batch 831/832---------[26/120000] (0d 10h 43m 10s) Loss_Di: 0.3616 Loss_Dv: 0.3771 Loss_Gi: 4.8868 Loss_Gv: 4.2193 Di_real_mean 0.8256 Di_fake_mean 0.0025 Dv_real_mean 0.8272 Dv_fake_mean 0.0103\n",
      "--------Batch 831/832---------[27/120000] (0d 11h 7m 52s) Loss_Di: 0.3566 Loss_Dv: 0.3536 Loss_Gi: 5.0272 Loss_Gv: 5.1914 Di_real_mean 0.9302 Di_fake_mean 0.0087 Dv_real_mean 0.9203 Dv_fake_mean 0.0097\n",
      "--------Batch 831/832---------[28/120000] (0d 11h 32m 33s) Loss_Di: 0.3445 Loss_Dv: 0.3851 Loss_Gi: 4.8211 Loss_Gv: 4.3794 Di_real_mean 0.8737 Di_fake_mean 0.0058 Dv_real_mean 0.8061 Dv_fake_mean 0.0044\n",
      "--------Batch 831/832---------[29/120000] (0d 11h 57m 15s) Loss_Di: 0.3612 Loss_Dv: 0.3742 Loss_Gi: 5.5216 Loss_Gv: 5.2027 Di_real_mean 0.8896 Di_fake_mean 0.0049 Dv_real_mean 0.9185 Dv_fake_mean 0.0128\n",
      "--------Batch 831/832---------[30/120000] (0d 12h 21m 58s) Loss_Di: 0.3917 Loss_Dv: 0.3856 Loss_Gi: 9.2208 Loss_Gv: 4.1275 Di_real_mean 0.8408 Di_fake_mean 0.0001 Dv_real_mean 0.8209 Dv_fake_mean 0.0122\n",
      "--------Batch 831/832---------[31/120000] (0d 12h 46m 40s) Loss_Di: 0.3430 Loss_Dv: 0.3696 Loss_Gi: 5.6026 Loss_Gv: 3.9781 Di_real_mean 0.8955 Di_fake_mean 0.0039 Dv_real_mean 0.8645 Dv_fake_mean 0.0127\n",
      "--------Batch 831/832---------[32/120000] (0d 13h 11m 21s) Loss_Di: 0.4599 Loss_Dv: 0.3991 Loss_Gi: 3.8467 Loss_Gv: 4.1354 Di_real_mean 0.7029 Di_fake_mean 0.0014 Dv_real_mean 0.7765 Dv_fake_mean 0.0038\n",
      "--------Batch 831/832---------[33/120000] (0d 13h 36m 3s) Loss_Di: 0.3520 Loss_Dv: 0.3625 Loss_Gi: 6.0498 Loss_Gv: 5.0318 Di_real_mean 0.9308 Di_fake_mean 0.0076 Dv_real_mean 0.8806 Dv_fake_mean 0.0058\n",
      "--------Batch 831/832---------[34/120000] (0d 14h 0m 44s) Loss_Di: 0.3890 Loss_Dv: 0.4026 Loss_Gi: 4.0183 Loss_Gv: 4.8594 Di_real_mean 0.9434 Di_fake_mean 0.0357 Dv_real_mean 0.9528 Dv_fake_mean 0.0172\n",
      "--------Batch 831/832---------[35/120000] (0d 14h 25m 25s) Loss_Di: 0.3970 Loss_Dv: 0.3910 Loss_Gi: 4.8192 Loss_Gv: 4.0453 Di_real_mean 0.8045 Di_fake_mean 0.0057 Dv_real_mean 0.9149 Dv_fake_mean 0.0485\n",
      "--------Batch 831/832---------[36/120000] (0d 14h 50m 6s) Loss_Di: 0.3621 Loss_Dv: 0.3700 Loss_Gi: 4.7592 Loss_Gv: 11.0711 Di_real_mean 0.8473 Di_fake_mean 0.0143 Dv_real_mean 0.8757 Dv_fake_mean 0.0001\n",
      "--------Batch 831/832---------[37/120000] (0d 15h 14m 48s) Loss_Di: 0.4452 Loss_Dv: 0.3904 Loss_Gi: 4.7256 Loss_Gv: 3.5496 Di_real_mean 0.9098 Di_fake_mean 0.0203 Dv_real_mean 0.9069 Dv_fake_mean 0.0523\n",
      "--------Batch 831/832---------[38/120000] (0d 15h 39m 29s) Loss_Di: 0.3392 Loss_Dv: 0.3546 Loss_Gi: 5.7236 Loss_Gv: 4.8841 Di_real_mean 0.8778 Di_fake_mean 0.0026 Dv_real_mean 0.8348 Dv_fake_mean 0.0052\n",
      "--------Batch 831/832---------[39/120000] (0d 16h 4m 10s) Loss_Di: 0.3508 Loss_Dv: 0.3532 Loss_Gi: 5.2777 Loss_Gv: 4.9436 Di_real_mean 0.9355 Di_fake_mean 0.0068 Dv_real_mean 0.9321 Dv_fake_mean 0.0126\n",
      "--------Batch 831/832---------[40/120000] (0d 16h 28m 52s) Loss_Di: 0.3391 Loss_Dv: 0.3416 Loss_Gi: 5.8156 Loss_Gv: 6.3472 Di_real_mean 0.8996 Di_fake_mean 0.0023 Dv_real_mean 0.8798 Dv_fake_mean 0.0009\n",
      "--------Batch 831/832---------[41/120000] (0d 16h 53m 34s) Loss_Di: 0.3322 Loss_Dv: 0.3399 Loss_Gi: 6.2669 Loss_Gv: 6.2321 Di_real_mean 0.8775 Di_fake_mean 0.0010 Dv_real_mean 0.9163 Dv_fake_mean 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Batch 831/832---------[42/120000] (0d 17h 18m 12s) Loss_Di: 0.3401 Loss_Dv: 0.3345 Loss_Gi: 5.9775 Loss_Gv: 6.2302 Di_real_mean 0.8699 Di_fake_mean 0.0009 Dv_real_mean 0.9063 Dv_fake_mean 0.0017\n",
      "--------Batch 831/832---------[43/120000] (0d 17h 42m 49s) Loss_Di: 0.3346 Loss_Dv: 0.3408 Loss_Gi: 6.0059 Loss_Gv: 5.5065 Di_real_mean 0.9247 Di_fake_mean 0.0018 Dv_real_mean 0.8693 Dv_fake_mean 0.0026\n",
      "--------Batch 831/832---------[44/120000] (0d 18h 7m 27s) Loss_Di: 0.3514 Loss_Dv: 0.3610 Loss_Gi: 4.6319 Loss_Gv: 5.5251 Di_real_mean 0.8684 Di_fake_mean 0.0038 Dv_real_mean 0.9494 Dv_fake_mean 0.0038\n",
      "--------Batch 831/832---------[45/120000] (0d 18h 32m 5s) Loss_Di: 0.3370 Loss_Dv: 0.3602 Loss_Gi: 6.9157 Loss_Gv: 4.9781 Di_real_mean 0.9202 Di_fake_mean 0.0011 Dv_real_mean 0.8534 Dv_fake_mean 0.0019\n",
      "--------Batch 831/832---------[46/120000] (0d 18h 56m 44s) Loss_Di: 0.3642 Loss_Dv: 0.3417 Loss_Gi: 5.5767 Loss_Gv: 6.3536 Di_real_mean 0.8321 Di_fake_mean 0.0025 Dv_real_mean 0.9028 Dv_fake_mean 0.0034\n",
      "--------Batch 831/832---------[47/120000] (0d 19h 21m 23s) Loss_Di: 0.3344 Loss_Dv: 0.3353 Loss_Gi: 5.5737 Loss_Gv: 5.8709 Di_real_mean 0.9133 Di_fake_mean 0.0038 Dv_real_mean 0.9130 Dv_fake_mean 0.0032\n",
      "--------Batch 831/832---------[48/120000] (0d 19h 46m 2s) Loss_Di: 0.3346 Loss_Dv: 0.3353 Loss_Gi: 7.9505 Loss_Gv: 6.1161 Di_real_mean 0.8708 Di_fake_mean 0.0002 Dv_real_mean 0.8869 Dv_fake_mean 0.0014\n",
      "--------Batch 831/832---------[49/120000] (0d 20h 10m 41s) Loss_Di: 0.3379 Loss_Dv: 0.3597 Loss_Gi: 6.7826 Loss_Gv: 5.5974 Di_real_mean 0.9134 Di_fake_mean 0.0011 Dv_real_mean 0.8953 Dv_fake_mean 0.0021\n",
      "--------Batch 831/832---------[50/120000] (0d 20h 35m 21s) Loss_Di: 0.3367 Loss_Dv: 0.3344 Loss_Gi: 5.6377 Loss_Gv: 5.9651 Di_real_mean 0.8984 Di_fake_mean 0.0033 Dv_real_mean 0.9069 Dv_fake_mean 0.0043\n",
      "--------Batch 831/832---------[51/120000] (0d 21h 0m 2s) Loss_Di: 0.3513 Loss_Dv: 0.3388 Loss_Gi: 6.2082 Loss_Gv: 6.7948 Di_real_mean 0.9166 Di_fake_mean 0.0023 Dv_real_mean 0.9133 Dv_fake_mean 0.0010\n",
      "--------Batch 831/832---------[52/120000] (0d 21h 24m 47s) Loss_Di: 0.3517 Loss_Dv: 0.3811 Loss_Gi: 4.9450 Loss_Gv: 5.3361 Di_real_mean 0.9081 Di_fake_mean 0.0083 Dv_real_mean 0.8601 Dv_fake_mean 0.0028\n",
      "--------Batch 831/832---------[53/120000] (0d 21h 49m 36s) Loss_Di: 0.3470 Loss_Dv: 0.3551 Loss_Gi: 6.4008 Loss_Gv: 5.6724 Di_real_mean 0.8836 Di_fake_mean 0.0013 Dv_real_mean 0.9391 Dv_fake_mean 0.0053\n",
      "--------Batch 831/832---------[54/120000] (0d 22h 14m 23s) Loss_Di: 0.3443 Loss_Dv: 0.3539 Loss_Gi: 6.0989 Loss_Gv: 4.8803 Di_real_mean 0.8852 Di_fake_mean 0.0013 Dv_real_mean 0.8790 Dv_fake_mean 0.0076\n",
      "--------Batch 831/832---------[55/120000] (0d 22h 39m 3s) Loss_Di: 0.3320 Loss_Dv: 0.3415 Loss_Gi: 6.1430 Loss_Gv: 5.7240 Di_real_mean 0.8824 Di_fake_mean 0.0011 Dv_real_mean 0.8970 Dv_fake_mean 0.0025\n",
      "--------Batch 831/832---------[56/120000] (0d 23h 3m 42s) Loss_Di: 0.3472 Loss_Dv: 0.3560 Loss_Gi: 6.2129 Loss_Gv: 5.7665 Di_real_mean 0.8418 Di_fake_mean 0.0004 Dv_real_mean 0.8338 Dv_fake_mean 0.0011\n",
      "--------Batch 831/832---------[57/120000] (0d 23h 28m 22s) Loss_Di: 0.3622 Loss_Dv: 0.3403 Loss_Gi: 5.8676 Loss_Gv: 6.7601 Di_real_mean 0.8060 Di_fake_mean 0.0004 Dv_real_mean 0.8516 Dv_fake_mean 0.0005\n",
      "--------Batch 831/832---------[58/120000] (0d 23h 53m 2s) Loss_Di: 0.3308 Loss_Dv: 0.3924 Loss_Gi: 6.8321 Loss_Gv: 2.0881 Di_real_mean 0.8744 Di_fake_mean 0.0006 Dv_real_mean 0.7745 Dv_fake_mean 0.0014\n",
      "--------Batch 831/832---------[59/120000] (1d 0h 17m 51s) Loss_Di: 0.3367 Loss_Dv: 0.3728 Loss_Gi: 5.4574 Loss_Gv: 4.6218 Di_real_mean 0.8957 Di_fake_mean 0.0027 Dv_real_mean 0.9213 Dv_fake_mean 0.0131\n",
      "--------Batch 831/832---------[60/120000] (1d 0h 42m 45s) Loss_Di: 0.3386 Loss_Dv: 0.3603 Loss_Gi: 6.7723 Loss_Gv: 6.3179 Di_real_mean 0.8796 Di_fake_mean 0.0007 Dv_real_mean 0.8198 Dv_fake_mean 0.0009\n",
      "--------Batch 598/832---------"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 21654, 21655, 21656, 21657, 21658, 21659, 21660, 21661) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a9693ee7e205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b8d2dadf2673>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mreal_videos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\r--------Batch {data_i}/{data_len}---------\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 21654, 21655, 21656, 21657, 21658, 21659, 21660, 21661) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "#load()\n",
    "train() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
