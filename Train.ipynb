{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of MoCoGAN\n",
    "---\n",
    "In this Notebook is covered the topic of training the MoCoGAN and the fine tuning of this network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining global variables and imports\n",
    "---\n",
    "In the below cell all of the imports that are needed to the training and some global variables like `device` to use the GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import skvideo.io\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim, cuda as cu, device, manual_seed\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torchvision.transforms import Lambda, Compose\n",
    "\n",
    "'''Import variables from train.py'''\n",
    "img_size = 96\n",
    "nc = 3\n",
    "ndf = 64 # from dcgan\n",
    "ngf = 64\n",
    "d_E = 10\n",
    "hidden_size = 100 # guess\n",
    "d_C = 50\n",
    "d_M = d_E\n",
    "nz  = d_C + d_M\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "T = 16 # Hyperparameter for taking #Frames into discriminator.\n",
    "\n",
    "ngpu       = 1\n",
    "batch_size = 16\n",
    "n_iter     = 120000\n",
    "pre_train  = False\n",
    "\n",
    "## Addition for training on UCF-101\n",
    "n_epochs_saveV      = 10\n",
    "n_epochs_display    = 1\n",
    "n_epochs_check      = 10\n",
    "max_frame           = 25\n",
    "cuda                = True #For compatibility with old version of MocoGan\n",
    "#### End of additions\n",
    "\n",
    "\n",
    "seed = 0\n",
    "manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = device(\"cuda\" if cu.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of Models\n",
    "From the `models` module, let's import all of the models and let's load the previous state for fine tuning.\n",
    "\n",
    "Then models are moved into the device chosen in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./mocogan/\")\n",
    "from models import Discriminator_I, Discriminator_V, Generator_I ,GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./mocogan/models.py:155: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(params)\n",
      "./mocogan/models.py:165: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(params, 0)\n"
     ]
    }
   ],
   "source": [
    "'''Create the objects for Discriminator_(I|V), GRU and Generator_I'''\n",
    "gen_i = Generator_I(nc, ngf, nz, ngpu = 1)\n",
    "gru = GRU(d_E, hidden_size, gpu = cu.is_available())\n",
    "dis_i = Discriminator_I(nc, ndf, ngpu = 1)\n",
    "dis_v = Discriminator_V(nc, ndf, T = T, ngpu = 1)\n",
    "gru.initWeight()\n",
    "\n",
    "'''Move objects into the device chosen'''\n",
    "''' adjust to cuda '''\n",
    "if cuda == True:\n",
    "    dis_i.cuda()\n",
    "    dis_v.cuda()\n",
    "    gen_i.cuda()\n",
    "    gru.cuda()\n",
    "    criterion.cuda()\n",
    "\n",
    "'''Optimizer Settings and Optimizer'''\n",
    "lr = 0.0002\n",
    "betas=(0.5, 0.999)\n",
    "optim_Di  = optim.Adam(dis_i.parameters(), lr=lr, betas=betas)\n",
    "optim_Dv  = optim.Adam(dis_v.parameters(), lr=lr, betas=betas)\n",
    "optim_Gi  = optim.Adam(gen_i.parameters(), lr=lr, betas=betas)\n",
    "optim_GRU = optim.Adam(gru.parameters(),   lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "---\n",
    "In the cells below, the dataloader will be defined and also all of the transformation that will be applied to the videos before taking them into a batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = !pwd\n",
    "current_path = str(current_path[0])\n",
    "resized_path = os.path.join(current_path, \"mocogan\", 'resized_data')\n",
    "files = glob(resized_path+'/*/*')\n",
    "\n",
    "#transformation = Compose([ToTensor(), Lambda(lambda tensor: (tensor - tensor.mean() )/ tensor.std())])\n",
    "\n",
    "transformation = Compose([Lambda(lambda video: video.transpose(3, 0, 1, 2)/255.0),\n",
    "                            Lambda(lambda video: video[ : , : max_frame, :, : ]),\n",
    "                            Lambda(lambda video: torch.FloatTensor(video))])\n",
    "\n",
    "filenameDictClassesIdx = \"classInd.txt\"\n",
    "dictClassesIdx = {}\n",
    "\n",
    "with open(os.path.join(current_path, \"ucfTrainTestlist\", filenameDictClassesIdx)) as file:\n",
    "    for line in file:\n",
    "        dictClassesIdx[ line.split() [1]] = int( line.split() [0] )\n",
    "\n",
    "dataset = DatasetFolder(resized_path, skvideo.io.vread, [\"mp4\"], transform= transformation)\n",
    "dataset.class_to_idx = dictClassesIdx\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size= batch_size, shuffle= True, num_workers= 8, pin_memory= True, drop_last= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "---\n",
    "\n",
    "In the cells below, first some methods to generate the Noise needed for the GAN is defined, then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Utilities'''\n",
    "def trim(video):\n",
    "    start = np.random.randint(0, video.shape[1] - (T+1))\n",
    "    end = start + T\n",
    "    return video[:, start:end, :, :]\n",
    "\n",
    "# for input noises to generate fake video\n",
    "# note that noises are trimmed randomly from n_frames to T for efficiency\n",
    "def trim_noise(noise):\n",
    "    #print(\"-----TRIMMING NOISE-----\")\n",
    "    #print(f\"Noise Size: {noise.size()}\")\n",
    "    start = np.random.randint(0, noise.size(1) - (T+1))\n",
    "    end = start + T\n",
    "    #print(\"-----END OF TRIMMING NOISE-----\")\n",
    "    return noise[:, start:end, :, :, :]\n",
    "\n",
    "\n",
    "''' calc grad of models '''\n",
    "\n",
    "def bp_i(inputs, y, retain=False):\n",
    "    label = (torch.FloatTensor()).cuda()\n",
    "    label.resize_(inputs.size(0)).fill_(y)\n",
    "    labelv = Variable(label)\n",
    "    outputs = dis_i(inputs)\n",
    "    err = criterion(outputs, labelv)\n",
    "    err.backward(retain_graph=retain)\n",
    "    toReturnErr = err.data[0] if err.size() == torch.Tensor().size() else err.item()\n",
    "    return toReturnErr, outputs.data.mean()\n",
    "\n",
    "def bp_v(inputs, y, retain=False):\n",
    "    #print(\"----BackPropagate_V-----\")\n",
    "    #print(inputs.size())\n",
    "    label = (torch.FloatTensor()).cuda()\n",
    "    try:\n",
    "        label.resize_(inputs.size(0)).fill_(y)\n",
    "\n",
    "    except RuntimeError as _:\n",
    "        # Dimension of y does not allow to use fill_\n",
    "        assert(inputs.size(0) == y.size(0))\n",
    "        label = (torch.FloatTensor(y)).cuda()\n",
    "\n",
    "    labelv = Variable(label)\n",
    "    outputs = dis_v(inputs)\n",
    "    err = criterion(outputs, labelv)\n",
    "    err.backward(retain_graph=retain)\n",
    "    toReturnErr = err.data[0] if err.size() == torch.Tensor().size() else err.item()\n",
    "    #print(\"----End of BackPropagate_V-----\")\n",
    "    return toReturnErr, outputs.data.mean()\n",
    "\n",
    "\n",
    "''' gen input noise for fake video '''\n",
    "\n",
    "def gen_z(n_frames, batch_size = batch_size):\n",
    "    #print(\"----Generating Z-----\")\n",
    "    #print(f\"N_FRAMES: {n_frames}\")\n",
    "    #print(f\"BATCH_SIZE: {batch_size}\")\n",
    "    #print(f\"D_C: {d_C}\")\n",
    "    #print(f\"D_E: {d_E}\")\n",
    "    #print(f\"nz: {nz}\")\n",
    "    z_C = Variable(torch.randn(batch_size, d_C))\n",
    "    #  repeat z_C to (batch_size, n_frames, d_C)\n",
    "    z_C = z_C.unsqueeze(1).repeat(1, n_frames, 1)\n",
    "    eps = Variable(torch.randn(batch_size, d_E))\n",
    "    if cuda == True:\n",
    "        z_C, eps = z_C.cuda(), eps.cuda()\n",
    "\n",
    "    gru.initHidden(batch_size)\n",
    "    # notice that 1st dim of gru outputs is seq_len, 2nd is batch_size\n",
    "    z_M = gru(eps, n_frames).transpose(1, 0)\n",
    "    z = torch.cat((z_M, z_C), 2)  # z.size() => (batch_size, n_frames, nz)\n",
    "    #print(\"----End Generating Z-----\")\n",
    "    return z.view(batch_size, n_frames, nz, 1, 1)\n",
    "\n",
    "''' prepare for train '''\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    d = math.floor(s / ((60**2)*24))\n",
    "    h = math.floor(s / (60**2)) - d*24\n",
    "    m = math.floor(s / 60) - h*60 - d*24*60\n",
    "    s = s - m*60 - h*(60**2) - d*24*(60**2)\n",
    "    return '%dd %dh %dm %ds' % (d, h, m, s)\n",
    "\n",
    "trained_path = os.path.join(current_path, 'trained_models')\n",
    "def checkpoint(model, optimizer, epoch):\n",
    "    filename = os.path.join(trained_path, '%s_epoch-%d' % (model.__class__.__name__, epoch))\n",
    "    torch.save(model.state_dict(), filename + '.model')\n",
    "    torch.save(optimizer.state_dict(), filename + '.state')\n",
    "\n",
    "def save_video(fake_video, epoch):\n",
    "    outputdata = fake_video * 255\n",
    "    outputdata = outputdata.astype(np.uint8)\n",
    "    dir_path = os.path.join(current_path, 'generated_videos')\n",
    "    file_path = os.path.join(dir_path, 'fakeVideo_epoch-%d.mp4' % epoch)\n",
    "    skvideo.io.vwrite(file_path, outputdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' train models '''\n",
    "def train():\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Starting training: CUDA is { 'On' if cuda == True else 'Off'}\")\n",
    "\n",
    "    for epoch in range(1, n_iter+1):\n",
    "        ''' prepare real images '''\n",
    "        # real_videos.size() => (batch_size, nc, T, img_size, img_size)\n",
    "\n",
    "        # Get data iterator\n",
    "        data_iter = iter(dataloader) #Iterator\n",
    "        data_len = len(dataloader) #Num Batches\n",
    "        data_i = 0\n",
    "\n",
    "        processedClass = None\n",
    "\n",
    "        while data_i < data_len:\n",
    "\n",
    "            try:\n",
    "                batch_size = 16\n",
    "                (real_videos, labels) = next(data_iter) #random_choice()\n",
    "\n",
    "                ''' Process 1 video for each class while testing. '''\n",
    "                #if (labels in processedClass):\n",
    "                #    continue\n",
    "\n",
    "                #else:\n",
    "                #    processedClass.append(labels)\n",
    "                ''' Process only 1 video class'''\n",
    "                #if processedClass is None:\n",
    "                #    processedClass = labels.item()\n",
    "                #else:\n",
    "                #    if processedClass != labels.item():\n",
    "                #        continue\n",
    "\n",
    "                for (key, val) in dictClassesIdx.items():\n",
    "                    if ( val in labels.tolist() ):\n",
    "                        pass\n",
    "                        #print(key)\n",
    "\n",
    "                if cuda == True:\n",
    "                    real_videos = real_videos.cuda()\n",
    "\n",
    "                real_videos = Variable(real_videos)\n",
    "                real_img = real_videos[:, :, np.random.randint(0, T), :, :]\n",
    "\n",
    "                ''' prepare fake images '''\n",
    "                # note that n_frames is sampled from video length distribution\n",
    "                n_frames = T + 2 + np.random.randint(0, real_videos.size()[2]) #video_lengths[np.random.randint(0, n_videos)]\n",
    "                Z = gen_z(n_frames, batch_size)  # Z.size() => (batch_size, n_frames, nz, 1, 1)\n",
    "                # trim => (batch_size, T, nz, 1, 1)\n",
    "                Z = trim_noise(Z)\n",
    "                # generate videos\n",
    "                Z = Z.contiguous().view(batch_size*T, nz, 1, 1)\n",
    "                fake_videos = gen_i(Z)\n",
    "                fake_videos = fake_videos.view(batch_size, T, nc, img_size, img_size)\n",
    "                # transpose => (batch_size, nc, T, img_size, img_size)\n",
    "                fake_videos = fake_videos.transpose(2, 1)\n",
    "                # img sampling\n",
    "                fake_img = fake_videos[:, :, np.random.randint(0, T), :, :]\n",
    "\n",
    "                ''' train discriminators '''\n",
    "                # video\n",
    "                dis_v.zero_grad()\n",
    "                randomStartFrameIdx = np.random.randint(0, real_videos.size()[2] - T - 1)\n",
    "                #print(\"-----INFOS-----\")\n",
    "                #print(f\"RandomStartFrame:{randomStartFrameIdx}\")\n",
    "                #print(f\"Video Size:{real_videos.size()}\")\n",
    "                #print(\"-----END OF INFOS-----\")\n",
    "                croppedRealVideos = real_videos[:,:,randomStartFrameIdx: randomStartFrameIdx + T, :, :]\n",
    "                #err_Dv_real, Dv_real_mean = bp_v(croppedRealVideos, 0.9)\n",
    "                err_Dv_real, Dv_real_mean = bp_v(croppedRealVideos, labels.type(torch.FloatTensor) / len(dictClassesIdx))\n",
    "                err_Dv_fake, Dv_fake_mean = bp_v(fake_videos.detach(), 0)\n",
    "                err_Dv = err_Dv_real + err_Dv_fake\n",
    "                optim_Dv.step()\n",
    "                # image\n",
    "                dis_i.zero_grad()\n",
    "                err_Di_real, Di_real_mean = bp_i(real_img, 0.9)\n",
    "                err_Di_fake, Di_fake_mean = bp_i(fake_img.detach(), 0)\n",
    "                err_Di = err_Di_real + err_Di_fake\n",
    "                optim_Di.step()\n",
    "\n",
    "\n",
    "                ''' train generators '''\n",
    "                gen_i.zero_grad()\n",
    "                gru.zero_grad()\n",
    "                # video. notice retain=True for back prop twice\n",
    "                err_Gv, _ = bp_v(fake_videos, 0.9, retain=True)\n",
    "                # images\n",
    "                err_Gi, _ = bp_i(fake_img, 0.9)\n",
    "                optim_Gi.step()\n",
    "                optim_GRU.step()\n",
    "\n",
    "                '''Increment index for Batch'''\n",
    "                data_i = data_i + 1\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                save_video(fake_videos[0].data.cpu().numpy().transpose(1, 2, 3, 0), epoch)\n",
    "                checkpoint(dis_i, optim_Di, epoch)\n",
    "                checkpoint(dis_v, optim_Dv, epoch)\n",
    "                checkpoint(gen_i, optim_Gi, epoch)\n",
    "                checkpoint(gru,   optim_GRU, epoch)\n",
    "\n",
    "\n",
    "        if epoch % n_epochs_display == 0:\n",
    "            print('[%d/%d] (%s) Loss_Di: %.4f Loss_Dv: %.4f Loss_Gi: %.4f Loss_Gv: %.4f Di_real_mean %.4f Di_fake_mean %.4f Dv_real_mean %.4f Dv_fake_mean %.4f'\n",
    "                  % (epoch, n_iter, timeSince(start_time), err_Di, err_Dv, err_Gi, err_Gv, Di_real_mean, Di_fake_mean, Dv_real_mean, Dv_fake_mean))\n",
    "\n",
    "        if epoch % n_epochs_saveV == 0:\n",
    "            save_video(fake_videos[0].data.cpu().numpy().transpose(1, 2, 3, 0), epoch)\n",
    "\n",
    "        if epoch % n_epochs_check == 0:\n",
    "            checkpoint(dis_i, optim_Di, epoch)\n",
    "            checkpoint(dis_v, optim_Dv, epoch)\n",
    "            checkpoint(gen_i, optim_Gi, epoch)\n",
    "            checkpoint(gru,   optim_GRU, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training: CUDA is On\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Cannot write to directory: /home/carlo/Documents/Cognitive Computing/Text2VideoGAN/generated_videos",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ac8976204ec3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m#err_Dv_real, Dv_real_mean = bp_v(croppedRealVideos, 0.9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0merr_Dv_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDv_real_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbp_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcroppedRealVideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictClassesIdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0merr_Dv_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDv_fake_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbp_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_videos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8b7f25b6fa3a>\u001b[0m in \u001b[0;36mbp_v\u001b[0;34m(inputs, y, retain)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2026\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2027\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-ac8976204ec3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0msave_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_videos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_Di\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_Dv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8b7f25b6fa3a>\u001b[0m in \u001b[0;36msave_video\u001b[0;34m(fake_video, epoch)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mdir_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'generated_videos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fakeVideo_epoch-%d.mp4'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mskvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skvideo/io/io.py\u001b[0m in \u001b[0;36mvwrite\u001b[0;34m(fname, videodata, inputdict, outputdict, backend, verbosity)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0m_HAS_FFMPEG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot find installation of real FFmpeg (which comes with ffprobe).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFFmpegWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideodata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skvideo/io/ffmpeg.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0m_HAS_FFMPEG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot find installation of real FFmpeg (which comes with ffprobe).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFFmpegWriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getSupportedEncoders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skvideo/io/abstract.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, inputdict, outputdict, verbosity)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# check to see if filename is a valid file location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_OK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot write to directory: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbasepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minputdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Cannot write to directory: /home/carlo/Documents/Cognitive Computing/Text2VideoGAN/generated_videos"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9acbc14d4fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m''' prepare dataset '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mcurrent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mresized_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'resized_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/*/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import skvideo.io\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torchvision.transforms import Lambda, Compose\n",
    "\n",
    "from models import Discriminator_I, Discriminator_V, Generator_I, GRU\n",
    "\n",
    "\n",
    "EXPLORATORY_DATA_ANALYSIS = False\n",
    "\n",
    "\n",
    "cuda       = True\n",
    "ngpu       = 1\n",
    "batch_size = 16\n",
    "n_iter     = 120000\n",
    "pre_train  = False\n",
    "\n",
    "## Addition for training on UCF-101\n",
    "n_epochs_saveV      = 10\n",
    "n_epochs_display    = 1\n",
    "n_epochs_check      = 10\n",
    "max_frame           = 25\n",
    "#### End of additions\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "if cuda == True:\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "\n",
    "''' prepare dataset '''\n",
    "\n",
    "current_path = !pwd\n",
    "current_path= \n",
    "resized_path = os.path.join(current_path, 'resized_data')\n",
    "files = glob.glob(resized_path+'/*/*')\n",
    "\n",
    "#transformation = Compose([ToTensor(), Lambda(lambda tensor: (tensor - tensor.mean() )/ tensor.std())])\n",
    "\n",
    "transformation = Compose([Lambda(lambda video: video.transpose(3, 0, 1, 2)/255.0),\n",
    "                            Lambda(lambda video: video[ : , : max_frame, :, : ]),\n",
    "                            Lambda(lambda video: torch.FloatTensor(video))])\n",
    "\n",
    "filenameDictClassesIdx = \"classInd.txt\"\n",
    "dictClassesIdx = {}\n",
    "\n",
    "with open(os.path.join(current_path, \"ucfTrainTestlist\", filenameDictClassesIdx)) as file:\n",
    "    for line in file:\n",
    "        dictClassesIdx[ line.split() [1]] = int( line.split() [0] )\n",
    "\n",
    "dataset = DatasetFolder(resized_path, skvideo.io.vread, [\"mp4\"], transform= transformation)\n",
    "dataset.class_to_idx = dictClassesIdx\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size= batch_size, shuffle= True, num_workers= 4, pin_memory= True, drop_last= True)\n",
    "\n",
    "\n",
    "if (EXPLORATORY_DATA_ANALYSIS):\n",
    "\n",
    "    minimum = 12000\n",
    "    #for line in files:\n",
    "        #print (f\"Started loading one of {len(files)} videos into memory... It will be fast.\")\n",
    "    video = skvideo.io.vread(files[0])\n",
    "    print(str(video.shape))\n",
    "    video = video.transpose(3, 0, 1 ,2) / 255.0\n",
    "    video = torch.FloatTensor(video)\n",
    "    print(str(video.shape))\n",
    "    #print (f\"Ended transforming into tensor: size is { (video.nelement() * video.element_size()) / (1024 * 1024)} MB\")\n",
    "    #print (f\"Final memory would be about {(video.nelement() * video.element_size()) / (1024 * 1024 * 1024) * len(files)} GB\")\n",
    "    # transpose each video to (nc, n_frames, img_size, img_size), and devide by 255\n",
    "    if (video.shape[0] < minimum ):\n",
    "        minimum = video.shape[0] #Minimum is 180\n",
    "\n",
    "\n",
    "    print(minimum)\n",
    "\n",
    "    #    video = video.transpose(3, 0, 1, 2) / 255.0\n",
    "\n",
    "\n",
    "\n",
    "''' prepare video sampling '''\n",
    "\n",
    "n_videos = len(dataset) #len(videos)\n",
    "T = 16\n",
    "\n",
    "# for true video\n",
    "def trim(video):\n",
    "    start = np.random.randint(0, video.shape[1] - (T+1))\n",
    "    end = start + T\n",
    "    return video[:, start:end, :, :]\n",
    "\n",
    "# for input noises to generate fake video\n",
    "# note that noises are trimmed randomly from n_frames to T for efficiency\n",
    "def trim_noise(noise):\n",
    "    #print(\"-----TRIMMING NOISE-----\")\n",
    "    #print(f\"Noise Size: {noise.size()}\")\n",
    "    start = np.random.randint(0, noise.size(1) - (T+1))\n",
    "    end = start + T\n",
    "    #print(\"-----END OF TRIMMING NOISE-----\")\n",
    "    return noise[:, start:end, :, :, :]\n",
    "\n",
    "def random_choice():\n",
    "    X = []\n",
    "    for _ in range(batch_size):\n",
    "        video = videos[np.random.randint(0, n_videos-1)]\n",
    "        video = torch.Tensor(trim(video))\n",
    "        X.append(video)\n",
    "    X = torch.stack(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "# video length distribution\n",
    "#video_lengths = [video.shape[1] for video in videos]\n",
    "\n",
    "\n",
    "''' set models '''\n",
    "\n",
    "img_size = 96\n",
    "nc = 3\n",
    "ndf = 64 # from dcgan\n",
    "ngf = 64\n",
    "d_E = 10\n",
    "hidden_size = 100 # guess\n",
    "d_C = 50\n",
    "d_M = d_E\n",
    "nz  = d_C + d_M\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "dis_i = Discriminator_I(nc, ndf, ngpu=ngpu)\n",
    "dis_v = Discriminator_V(nc, ndf, T=T, ngpu=ngpu)\n",
    "gen_i = Generator_I(nc, ngf, nz, ngpu=ngpu)\n",
    "gru = GRU(d_E, hidden_size, gpu=cuda)\n",
    "gru.initWeight()\n",
    "\n",
    "\n",
    "''' prepare for train '''\n",
    "\n",
    "#label = torch.FloatTensor()\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    d = math.floor(s / ((60**2)*24))\n",
    "    h = math.floor(s / (60**2)) - d*24\n",
    "    m = math.floor(s / 60) - h*60 - d*24*60\n",
    "    s = s - m*60 - h*(60**2) - d*24*(60**2)\n",
    "    return '%dd %dh %dm %ds' % (d, h, m, s)\n",
    "\n",
    "trained_path = os.path.join(current_path, 'trained_models')\n",
    "def checkpoint(model, optimizer, epoch):\n",
    "    filename = os.path.join(trained_path, '%s_epoch-%d' % (model.__class__.__name__, epoch))\n",
    "    torch.save(model.state_dict(), filename + '.model')\n",
    "    torch.save(optimizer.state_dict(), filename + '.state')\n",
    "\n",
    "def save_video(fake_video, epoch):\n",
    "    outputdata = fake_video * 255\n",
    "    outputdata = outputdata.astype(np.uint8)\n",
    "    dir_path = os.path.join(current_path, 'generated_videos')\n",
    "    file_path = os.path.join(dir_path, 'fakeVideo_epoch-%d.mp4' % epoch)\n",
    "    skvideo.io.vwrite(file_path, outputdata)\n",
    "\n",
    "\n",
    "''' adjust to cuda '''\n",
    "\n",
    "if cuda == True:\n",
    "    dis_i.cuda()\n",
    "    dis_v.cuda()\n",
    "    gen_i.cuda()\n",
    "    gru.cuda()\n",
    "    criterion.cuda()\n",
    "    #label = label.cuda()\n",
    "\n",
    "\n",
    "# setup optimizer\n",
    "lr = 0.0002\n",
    "betas=(0.5, 0.999)\n",
    "optim_Di  = optim.Adam(dis_i.parameters(), lr=lr, betas=betas)\n",
    "optim_Dv  = optim.Adam(dis_v.parameters(), lr=lr, betas=betas)\n",
    "optim_Gi  = optim.Adam(gen_i.parameters(), lr=lr, betas=betas)\n",
    "optim_GRU = optim.Adam(gru.parameters(),   lr=lr, betas=betas)\n",
    "\n",
    "\n",
    "''' use pre-trained models '''\n",
    "\n",
    "if pre_train == True:\n",
    "    dis_i.load_state_dict(torch.load(trained_path + '/Discriminator_I.model'))\n",
    "    dis_v.load_state_dict(torch.load(trained_path + '/Discriminator_V.model'))\n",
    "    gen_i.load_state_dict(torch.load(trained_path + '/Generator_I.model'))\n",
    "    gru.load_state_dict(torch.load(trained_path + '/GRU.model'))\n",
    "    optim_Di.load_state_dict(torch.load(trained_path + '/Discriminator_I.state'))\n",
    "    optim_Dv.load_state_dict(torch.load(trained_path + '/Discriminator_V.state'))\n",
    "    optim_Gi.load_state_dict(torch.load(trained_path + '/Generator_I.state'))\n",
    "    optim_GRU.load_state_dict(torch.load(trained_path + '/GRU.state'))\n",
    "\n",
    "\n",
    "''' calc grad of models '''\n",
    "\n",
    "def bp_i(inputs, y, retain=False):\n",
    "    label = (torch.FloatTensor()).cuda()\n",
    "    label.resize_(inputs.size(0)).fill_(y)\n",
    "    labelv = Variable(label)\n",
    "    outputs = dis_i(inputs)\n",
    "    err = criterion(outputs, labelv)\n",
    "    err.backward(retain_graph=retain)\n",
    "    toReturnErr = err.data[0] if err.size() == torch.Tensor().size() else err.item()\n",
    "    return toReturnErr, outputs.data.mean()\n",
    "\n",
    "def bp_v(inputs, y, retain=False):\n",
    "    #print(\"----BackPropagate_V-----\")\n",
    "    #print(inputs.size())\n",
    "    label = (torch.FloatTensor()).cuda()\n",
    "    try:\n",
    "        label.resize_(inputs.size(0)).fill_(y)\n",
    "\n",
    "    except RuntimeError as _:\n",
    "        # Dimension of y does not allow to use fill_\n",
    "        assert(inputs.size(0) == y.size(0))\n",
    "        label = (torch.FloatTensor(y)).cuda()\n",
    "\n",
    "    labelv = Variable(label)\n",
    "    outputs = dis_v(inputs)\n",
    "    err = criterion(outputs, labelv)\n",
    "    err.backward(retain_graph=retain)\n",
    "    toReturnErr = err.data[0] if err.size() == torch.Tensor().size() else err.item()\n",
    "    #print(\"----End of BackPropagate_V-----\")\n",
    "    return toReturnErr, outputs.data.mean()\n",
    "\n",
    "\n",
    "''' gen input noise for fake video '''\n",
    "\n",
    "def gen_z(n_frames, batch_size = batch_size):\n",
    "    #print(\"----Generating Z-----\")\n",
    "    #print(f\"N_FRAMES: {n_frames}\")\n",
    "    #print(f\"BATCH_SIZE: {batch_size}\")\n",
    "    #print(f\"D_C: {d_C}\")\n",
    "    #print(f\"D_E: {d_E}\")\n",
    "    #print(f\"nz: {nz}\")\n",
    "    z_C = Variable(torch.randn(batch_size, d_C))\n",
    "    #  repeat z_C to (batch_size, n_frames, d_C)\n",
    "    z_C = z_C.unsqueeze(1).repeat(1, n_frames, 1)\n",
    "    eps = Variable(torch.randn(batch_size, d_E))\n",
    "    if cuda == True:\n",
    "        z_C, eps = z_C.cuda(), eps.cuda()\n",
    "\n",
    "    gru.initHidden(batch_size)\n",
    "    # notice that 1st dim of gru outputs is seq_len, 2nd is batch_size\n",
    "    z_M = gru(eps, n_frames).transpose(1, 0)\n",
    "    z = torch.cat((z_M, z_C), 2)  # z.size() => (batch_size, n_frames, nz)\n",
    "    #print(\"----End Generating Z-----\")\n",
    "    return z.view(batch_size, n_frames, nz, 1, 1)\n",
    "\n",
    "\n",
    "''' train models '''\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting training: CUDA is { 'On' if cuda == True else 'Off'}\")\n",
    "\n",
    "for epoch in range(1, n_iter+1):\n",
    "    ''' prepare real images '''\n",
    "    # real_videos.size() => (batch_size, nc, T, img_size, img_size)\n",
    "\n",
    "    # Get data iterator\n",
    "    data_iter = iter(dataloader) #Iterator\n",
    "    data_len = len(dataloader) #Num Batches\n",
    "    data_i = 0\n",
    "\n",
    "    processedClass = None\n",
    "\n",
    "    while data_i < data_len:\n",
    "\n",
    "        try:\n",
    "            batch_size = 16\n",
    "            (real_videos, labels) = next(data_iter) #random_choice()\n",
    "\n",
    "            ''' Process 1 video for each class while testing. '''\n",
    "            #if (labels in processedClass):\n",
    "            #    continue\n",
    "\n",
    "            #else:\n",
    "            #    processedClass.append(labels)\n",
    "            ''' Process only 1 video class'''\n",
    "            #if processedClass is None:\n",
    "            #    processedClass = labels.item()\n",
    "            #else:\n",
    "            #    if processedClass != labels.item():\n",
    "            #        continue\n",
    "\n",
    "            for (key, val) in dictClassesIdx.items():\n",
    "                if ( val in labels.tolist() ):\n",
    "                    pass\n",
    "                    #print(key)\n",
    "\n",
    "            if cuda == True:\n",
    "                real_videos = real_videos.cuda()\n",
    "\n",
    "            real_videos = Variable(real_videos)\n",
    "            real_img = real_videos[:, :, np.random.randint(0, T), :, :]\n",
    "\n",
    "            ''' prepare fake images '''\n",
    "            # note that n_frames is sampled from video length distribution\n",
    "            n_frames = T + 2 + np.random.randint(0, real_videos.size()[2]) #video_lengths[np.random.randint(0, n_videos)]\n",
    "            Z = gen_z(n_frames, batch_size)  # Z.size() => (batch_size, n_frames, nz, 1, 1)\n",
    "            # trim => (batch_size, T, nz, 1, 1)\n",
    "            Z = trim_noise(Z)\n",
    "            # generate videos\n",
    "            Z = Z.contiguous().view(batch_size*T, nz, 1, 1)\n",
    "            fake_videos = gen_i(Z)\n",
    "            fake_videos = fake_videos.view(batch_size, T, nc, img_size, img_size)\n",
    "            # transpose => (batch_size, nc, T, img_size, img_size)\n",
    "            fake_videos = fake_videos.transpose(2, 1)\n",
    "            # img sampling\n",
    "            fake_img = fake_videos[:, :, np.random.randint(0, T), :, :]\n",
    "\n",
    "            ''' train discriminators '''\n",
    "            # video\n",
    "            dis_v.zero_grad()\n",
    "            randomStartFrameIdx = np.random.randint(0, real_videos.size()[2] - T - 1)\n",
    "            #print(\"-----INFOS-----\")\n",
    "            #print(f\"RandomStartFrame:{randomStartFrameIdx}\")\n",
    "            #print(f\"Video Size:{real_videos.size()}\")\n",
    "            #print(\"-----END OF INFOS-----\")\n",
    "            croppedRealVideos = real_videos[:,:,randomStartFrameIdx: randomStartFrameIdx + T, :, :]\n",
    "            #err_Dv_real, Dv_real_mean = bp_v(croppedRealVideos, 0.9)\n",
    "            err_Dv_real, Dv_real_mean = bp_v(croppedRealVideos, labels.type(torch.FloatTensor) / len(dictClassesIdx))\n",
    "            err_Dv_fake, Dv_fake_mean = bp_v(fake_videos.detach(), 0)\n",
    "            err_Dv = err_Dv_real + err_Dv_fake\n",
    "            optim_Dv.step()\n",
    "            # image\n",
    "            dis_i.zero_grad()\n",
    "            err_Di_real, Di_real_mean = bp_i(real_img, 0.9)\n",
    "            err_Di_fake, Di_fake_mean = bp_i(fake_img.detach(), 0)\n",
    "            err_Di = err_Di_real + err_Di_fake\n",
    "            optim_Di.step()\n",
    "\n",
    "\n",
    "            ''' train generators '''\n",
    "            gen_i.zero_grad()\n",
    "            gru.zero_grad()\n",
    "            # video. notice retain=True for back prop twice\n",
    "            err_Gv, _ = bp_v(fake_videos, 0.9, retain=True)\n",
    "            # images\n",
    "            err_Gi, _ = bp_i(fake_img, 0.9)\n",
    "            optim_Gi.step()\n",
    "            optim_GRU.step()\n",
    "\n",
    "            '''Increment index for Batch'''\n",
    "            data_i = data_i + 1\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            save_video(fake_videos[0].data.cpu().numpy().transpose(1, 2, 3, 0), epoch)\n",
    "            checkpoint(dis_i, optim_Di, epoch)\n",
    "            checkpoint(dis_v, optim_Dv, epoch)\n",
    "            checkpoint(gen_i, optim_Gi, epoch)\n",
    "            checkpoint(gru,   optim_GRU, epoch)\n",
    "\n",
    "\n",
    "    if epoch % n_epochs_display == 0:\n",
    "        print('[%d/%d] (%s) Loss_Di: %.4f Loss_Dv: %.4f Loss_Gi: %.4f Loss_Gv: %.4f Di_real_mean %.4f Di_fake_mean %.4f Dv_real_mean %.4f Dv_fake_mean %.4f'\n",
    "              % (epoch, n_iter, timeSince(start_time), err_Di, err_Dv, err_Gi, err_Gv, Di_real_mean, Di_fake_mean, Dv_real_mean, Dv_fake_mean))\n",
    "\n",
    "    if epoch % n_epochs_saveV == 0:\n",
    "        save_video(fake_videos[0].data.cpu().numpy().transpose(1, 2, 3, 0), epoch)\n",
    "\n",
    "    if epoch % n_epochs_check == 0:\n",
    "        checkpoint(dis_i, optim_Di, epoch)\n",
    "        checkpoint(dis_v, optim_Dv, epoch)\n",
    "        checkpoint(gen_i, optim_Gi, epoch)\n",
    "        checkpoint(gru,   optim_GRU, epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
